<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>An easier way to post</title>
    <url>/2020/02/10/An-easier-way-to-post/</url>
    <content><![CDATA[<p>When I have a new idea, the fastest way to post on the website is:<br>Open VSCODE -&gt; hexo new -&gt; Writing -&gt; hexo generator -d<br>It’s complex and not efficient enough. I’m developing a new and more efficient post method.<br>The initial idea is using scheduled script in my mac, execute it every 5 min (a suitable duration). By iCloud sync service, sync my file in the source folder.<br>In this situation, I just need to update the markdown file, the rest missions are for the script.</p>
]]></content>
  </entry>
  <entry>
    <title>First Post</title>
    <url>/2020/02/10/First-Post/</url>
    <content><![CDATA[<p>Welcome to my website. I will post some notes or thinking sparks on this website.<br>Today is Feb 02, 2020.<br>I am in Madison, WI, United State.</p>
<p>This semester I have five class. They are MATH 542, MATH 704, MATH 715, MATH 801 and STAT 615. It is a challenge for me, hope to learn them as best as I can.</p>
<p>I am looking for the chance to do some research this summer vacation.</p>
<p>I am interested in Applied Math, Machine Learning and Artificial Intelligence. Generally, I am interested the combination of Math and Computer Science. That’s the future, like Auto-polit, in my opinion.</p>
<p>Be a better man everyday!</p>
]]></content>
  </entry>
  <entry>
    <title>Kaggle入门(1)--Titanic</title>
    <url>/2019/04/09/Kaggle%E5%85%A5%E9%97%A8-1-Titanic/</url>
    <content><![CDATA[<p><code>Kaggle</code>是一个举办机器学习竞赛、托管数据库、编写和分享代码的平台。在这里可以学到很多数据科学的相关知识以及知识落地。</p>
<p>本次数据挖掘的目的是<code>给出泰坦尼克号上的乘客的信息, 预测乘客是否幸存</code></p>
<center>本文参考自 https://cloud.tencent.com/developer/article/1063994 </center>

<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><p>数据来源 : <code>https://www.kaggle.com/c/titanic/data</code><br>平台 : Python<br>涉及到的库 : <code>sklearn</code>, <code>numpy</code>, <code>seaborn</code>, <code>pandas</code>, …</p>
<h2 id="数据获取方法"><a href="#数据获取方法" class="headerlink" title="数据获取方法"></a>数据获取方法</h2><h3 id="方法1"><a href="#方法1" class="headerlink" title="方法1"></a>方法1</h3><p>网页下载，解压。</p>
<h3 id="方法2"><a href="#方法2" class="headerlink" title="方法2"></a>方法2</h3><p>使用<code>kaggle api</code>, <code>Github</code>地址<code>https://github.com/Kaggle/kaggle-api</code><br>安装方法 : <code>pip install kaggle</code><br>使用方法 : <code>kaggle competitions download -c titanic</code></p>
<a id="more"></a>
<h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><p>Titanic 生存模型预测，其中包含了两组数据：train.csv 和 test.csv，分别为训练集合和测试集合。</p>
<h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'test.csv'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="数据类型与特征信息概览"><a href="#数据类型与特征信息概览" class="headerlink" title="数据类型与特征信息概览"></a>数据类型与特征信息概览</h3><ol>
<li><p>预览<code>train.csv</code>的前几行数据并了解每一个<code>feature</code>的意义, 观察前几行的源数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.set_style(<span class="string">'whitegrid'</span>)</span><br><span class="line">train_data.head()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/train_data_head.png" alt="avatar"></p>
</li>
<li><p>每维<code>feature</code>的<code>info</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data.info()</span><br><span class="line">print(<span class="string">"-"</span> * <span class="number">40</span>)</span><br><span class="line">test_data.info()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/train_test_info.png" alt="avatar"><br>从上面我们可以看出，Age、Cabin、Embarked、Fare几个特征存在缺失值。</p>
</li>
<li><p>绘制存活的比例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[<span class="string">'Survived'</span>].value_counts().plot.pie(autopct = <span class="string">'%1.2f%%'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Survived_value_counts.png" alt="avatar"></p>
</li>
</ol>
<h3 id="缺失值处理方法"><a href="#缺失值处理方法" class="headerlink" title="缺失值处理方法"></a>缺失值处理方法</h3><p>有一部分的机器学习算法对缺失值比较敏感<br>处理方法有如下几种</p>
<ol>
<li>如果数据集很多，但有很少的缺失值，可以删掉带缺失值的行</li>
<li>如果该属性相对学习来说不是很重要，可以对缺失值赋均值或者众数。比如在哪儿上船<code>Embarked</code>这一属性(共有三个上船地点)，缺失俩值，可以用众数赋值<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data.Embarked[train_data.Embarked.isnull()] = train_data.Embarked.dropna().mode().values</span><br></pre></td></tr></table></figure></li>
<li>对于标称属性，可以赋一个代表缺失的值，比如’U0’。因为缺失本身也可能代表着一些隐含信息。比如船舱号<code>Cabin</code>这一属性，缺失可能代表并没有船舱<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#replace missing value with U0</span></span><br><span class="line">train_data[<span class="string">'Cabin'</span>] = train_data.Cabin.fillna(<span class="string">'U0'</span>) <span class="comment"># train_data.Cabin[train_data.Cabin.isnull()]='U0'</span></span><br></pre></td></tr></table></figure></li>
<li>使用<code>回归</code>、<code>随机森林</code>等模型来预测缺失属性的值<br>因为<code>Age</code>在该数据集里是一个相当重要的特征(先对Age进行分析即可得知)，所以保证一定的缺失值填充准确率是非常重要的，对结果也会产生较大影响。<br>一般情况下，会使用数据完整的条目作为模型的训练集，以此来预测缺失值。<br>对于当前的这个数据，可以使用<code>随机森林</code>来预测也可以使用<code>线性回归</code>预测。<br>这里使用<code>随机森林</code>预测模型，选取数据集中的数值属性作为特征。<br>因为<code>sklearn</code>的模型只能处理<code>数值属性</code>，所以这里先仅选取数值特征，但在实际的应用中需要将非数值特征转换为数值特征。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment">#choose training data to predict age</span></span><br><span class="line">age_df = train_data[[<span class="string">'Age'</span>,<span class="string">'Survived'</span>,<span class="string">'Fare'</span>, <span class="string">'Parch'</span>, <span class="string">'SibSp'</span>, <span class="string">'Pclass'</span>]]</span><br><span class="line">age_df_notnull = age_df.loc[(train_data[<span class="string">'Age'</span>].notnull())]</span><br><span class="line">age_df_isnull = age_df.loc[(train_data[<span class="string">'Age'</span>].isnull())]</span><br><span class="line">X = age_df_notnull.values[:,<span class="number">1</span>:]</span><br><span class="line">Y = age_df_notnull.values[:,<span class="number">0</span>]</span><br><span class="line"><span class="comment"># use RandomForestRegression to train data</span></span><br><span class="line">RFR = RandomForestRegressor(n_estimators=<span class="number">1000</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">RFR.fit(X,Y)</span><br><span class="line">predictAges = RFR.predict(age_df_isnull.values[:,<span class="number">1</span>:])</span><br><span class="line">train_data.loc[train_data[<span class="string">'Age'</span>].isnull(), [<span class="string">'Age'</span>]]= predictAges</span><br></pre></td></tr></table></figure>
再来看一下缺失数据处理后的DataFram<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data.info()</span><br></pre></td></tr></table></figure>
<img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/train_data_info.png" alt="avatar"></li>
</ol>
<h3 id="分析数据关系"><a href="#分析数据关系" class="headerlink" title="分析数据关系"></a>分析数据关系</h3><h4 id="性别与是否生存的关系-Sex"><a href="#性别与是否生存的关系-Sex" class="headerlink" title="性别与是否生存的关系 Sex"></a>性别与是否生存的关系 <code>Sex</code></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data.groupby([<span class="string">'Sex'</span>,<span class="string">'Survived'</span>])[<span class="string">'Survived'</span>].count()</span><br><span class="line">train_data[[<span class="string">'Sex'</span>,<span class="string">'Survived'</span>]].groupby([<span class="string">'Sex'</span>]).mean().plot.bar()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Sex_Survived1.png" alt="avatar"><br><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Sex_Survived2.png" alt="avatar"></p>
<p>以上为不同性别的生存率，可见在泰坦尼克号事故中, 体现了<code>Lady First</code>!</p>
<h4 id="船舱等级和生存与否的关系-Pclass"><a href="#船舱等级和生存与否的关系-Pclass" class="headerlink" title="船舱等级和生存与否的关系 Pclass"></a>船舱等级和生存与否的关系 <code>Pclass</code></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data.groupby([<span class="string">'Pclass'</span>,<span class="string">'Survived'</span>])[<span class="string">'Pclass'</span>].count()</span><br><span class="line">train_data[[<span class="string">'Pclass'</span>,<span class="string">'Survived'</span>]].groupby([<span class="string">'Pclass'</span>]).mean().plot.bar()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Pclass_Survived1.png" alt="avatar"><br><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Pclass_Survived2.png" alt="avatar"></p>
<p><code>不同等级船舱的男女生存率</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[[<span class="string">'Sex'</span>,<span class="string">'Pclass'</span>,<span class="string">'Survived'</span>]].groupby([<span class="string">'Pclass'</span>,<span class="string">'Sex'</span>]).mean().plot.bar()</span><br><span class="line">train_data.groupby([<span class="string">'Sex'</span>, <span class="string">'Pclass'</span>, <span class="string">'Survived'</span>])[<span class="string">'Survived'</span>].count()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Sex_Pclass_Survived1.png" alt="avatar"><br><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Sex_Pclass_Survived2.png" alt="avatar"></p>
<h4 id="年龄与存活与否的关系-Age"><a href="#年龄与存活与否的关系-Age" class="headerlink" title="年龄与存活与否的关系 Age"></a>年龄与存活与否的关系 <code>Age</code></h4><h5 id="分别分析不同等级船舱和不同性别下的年龄分布和生存的关系"><a href="#分别分析不同等级船舱和不同性别下的年龄分布和生存的关系" class="headerlink" title="分别分析不同等级船舱和不同性别下的年龄分布和生存的关系"></a>分别分析不同等级船舱和不同性别下的年龄分布和生存的关系</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize = (<span class="number">18</span>, <span class="number">8</span>))</span><br><span class="line">sns.violinplot(<span class="string">"Pclass"</span>, <span class="string">"Age"</span>, hue=<span class="string">"Survived"</span>, data=train_data, split=<span class="literal">True</span>, ax=ax[<span class="number">0</span>])</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">'Pclass and Age vs Survived'</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_yticks(range(<span class="number">0</span>, <span class="number">110</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">sns.violinplot(<span class="string">"Sex"</span>, <span class="string">"Age"</span>, hue=<span class="string">"Survived"</span>, data=train_data, split=<span class="literal">True</span>, ax=ax[<span class="number">1</span>])</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">'Sex and Age vs Survived'</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_yticks(range(<span class="number">0</span>, <span class="number">110</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Age_Survived1.png" alt="avatar"></p>
<h5 id="分析总体的年龄分布"><a href="#分析总体的年龄分布" class="headerlink" title="分析总体的年龄分布"></a>分析总体的年龄分布</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">train_data[<span class="string">'Age'</span>].hist(bins=<span class="number">70</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Age'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Num'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">train_data.boxplot(column=<span class="string">'Age'</span>, showfliers=<span class="literal">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Age_Survived2.png" alt="avatar"></p>
<h5 id="不同年龄下的生存和非生存的分布情况"><a href="#不同年龄下的生存和非生存的分布情况" class="headerlink" title="不同年龄下的生存和非生存的分布情况"></a>不同年龄下的生存和非生存的分布情况</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">facet = sns.FacetGrid(train_data, hue=<span class="string">"Survived"</span>,aspect=<span class="number">4</span>)</span><br><span class="line">facet.map(sns.kdeplot,<span class="string">'Age'</span>,shade= <span class="literal">True</span>)</span><br><span class="line">facet.set(xlim=(<span class="number">0</span>, train_data[<span class="string">'Age'</span>].max()))</span><br><span class="line">facet.add_legend()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Age_Survived3.png" alt="avatar"></p>
<h5 id="不同年龄下的平均生存率"><a href="#不同年龄下的平均生存率" class="headerlink" title="不同年龄下的平均生存率"></a>不同年龄下的平均生存率</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># average survived passengers by age</span></span><br><span class="line">fig, axis1 = plt.subplots(<span class="number">1</span>,<span class="number">1</span>,figsize=(<span class="number">18</span>,<span class="number">4</span>))</span><br><span class="line">train_data[<span class="string">"Age_int"</span>] = train_data[<span class="string">"Age"</span>].astype(int)</span><br><span class="line">average_age = train_data[[<span class="string">"Age_int"</span>, <span class="string">"Survived"</span>]].groupby([<span class="string">'Age_int'</span>],as_index=<span class="literal">False</span>).mean()</span><br><span class="line">sns.barplot(x=<span class="string">'Age_int'</span>, y=<span class="string">'Survived'</span>, data=average_age)</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Age_Survived4.png" alt="avatar"></p>
<h5 id="按照年龄，将乘客划分为儿童、少年、成年和老年，分析四个群体的生还情况"><a href="#按照年龄，将乘客划分为儿童、少年、成年和老年，分析四个群体的生还情况" class="headerlink" title="按照年龄，将乘客划分为儿童、少年、成年和老年，分析四个群体的生还情况"></a>按照年龄，将乘客划分为儿童、少年、成年和老年，分析四个群体的生还情况</h5><p>样本有891，平均年龄约为30岁，标准差13.5岁，最小年龄为0.42，最大年龄80</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[<span class="string">'Age'</span>].describe()</span><br><span class="line"></span><br><span class="line">bins = [<span class="number">0</span>, <span class="number">12</span>, <span class="number">18</span>, <span class="number">65</span>, <span class="number">100</span>]</span><br><span class="line">train_data[<span class="string">'Age_group'</span>] = pd.cut(train_data[<span class="string">'Age'</span>], bins)</span><br><span class="line">by_age = train_data.groupby(<span class="string">'Age_group'</span>)[<span class="string">'Survived'</span>].mean()</span><br><span class="line">by_age</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Age_Survived5.png" alt="avatar"><br><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Age_Survived6.png" alt="avatar"><br><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Age_Survived7.png" alt="avatar"></p>
<h4 id="称呼与存活与否的关系-Name"><a href="#称呼与存活与否的关系-Name" class="headerlink" title="称呼与存活与否的关系 Name"></a>称呼与存活与否的关系 <code>Name</code></h4><h5 id="通过观察名字数据，我们可以看出其中包括对乘客的称呼。称呼信息包含了乘客的年龄、性别，同时也包含了如社会地位等的信息。"><a href="#通过观察名字数据，我们可以看出其中包括对乘客的称呼。称呼信息包含了乘客的年龄、性别，同时也包含了如社会地位等的信息。" class="headerlink" title="通过观察名字数据，我们可以看出其中包括对乘客的称呼。称呼信息包含了乘客的年龄、性别，同时也包含了如社会地位等的信息。"></a>通过观察名字数据，我们可以看出其中包括对乘客的称呼。称呼信息包含了乘客的年龄、性别，同时也包含了如社会地位等的信息。</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[<span class="string">'Title'</span>] = train_data[<span class="string">'Name'</span>].str.extract(<span class="string">' ([A-Za-z]+)\.'</span>, expand=<span class="literal">False</span>)</span><br><span class="line">pd.crosstab(train_data[<span class="string">'Title'</span>], train_data[<span class="string">'Sex'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Name_List.png" alt="avatar"></p>
<h5 id="观察不同称呼与生存率的关系"><a href="#观察不同称呼与生存率的关系" class="headerlink" title="观察不同称呼与生存率的关系"></a>观察不同称呼与生存率的关系</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[[<span class="string">'Title'</span>,<span class="string">'Survived'</span>]].groupby([<span class="string">'Title'</span>]).mean().plot.bar()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Name_Survived.png" alt="avatar"></p>
<h5 id="观察名字长度和生存率之间存在关系的可能"><a href="#观察名字长度和生存率之间存在关系的可能" class="headerlink" title="观察名字长度和生存率之间存在关系的可能"></a>观察名字长度和生存率之间存在关系的可能</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig, axis1 = plt.subplots(<span class="number">1</span>,<span class="number">1</span>,figsize=(<span class="number">18</span>,<span class="number">4</span>))</span><br><span class="line">train_data[<span class="string">'Name_length'</span>] = train_data[<span class="string">'Name'</span>].apply(len)</span><br><span class="line">name_length = train_data[[<span class="string">'Name_length'</span>,<span class="string">'Survived'</span>]].groupby([<span class="string">'Name_length'</span>],as_index=<span class="literal">False</span>).mean()</span><br><span class="line">sns.barplot(x=<span class="string">'Name_length'</span>, y=<span class="string">'Survived'</span>, data=name_length)</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Name_Length_Survived.png" alt="avatar"><br>从上面的图片可以看出，名字长度和生存与否确实也存在一定的相关性。</p>
<h4 id="有无兄弟姐妹和存活与否的关系-SibSp"><a href="#有无兄弟姐妹和存活与否的关系-SibSp" class="headerlink" title="有无兄弟姐妹和存活与否的关系 SibSp"></a>有无兄弟姐妹和存活与否的关系 <code>SibSp</code></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将数据分为有兄弟姐妹的和没有兄弟姐妹的两组：</span></span><br><span class="line">sibsp_df = train_data[train_data[<span class="string">'SibSp'</span>] != <span class="number">0</span>]</span><br><span class="line">no_sibsp_df = train_data[train_data[<span class="string">'SibSp'</span>] == <span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">sibsp_df[<span class="string">'Survived'</span>].value_counts().plot.pie(labels=[<span class="string">'No Survived'</span>, <span class="string">'Survived'</span>], autopct = <span class="string">'%1.1f%%'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'sibsp'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">no_sibsp_df[<span class="string">'Survived'</span>].value_counts().plot.pie(labels=[<span class="string">'No Survived'</span>, <span class="string">'Survived'</span>], autopct = <span class="string">'%1.1f%%'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'no_sibsp'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/SibSp_Survived.png" alt="avatar"></p>
<h4 id="有无父母子女和存活与否的关系-Parch"><a href="#有无父母子女和存活与否的关系-Parch" class="headerlink" title="有无父母子女和存活与否的关系 Parch"></a>有无父母子女和存活与否的关系 <code>Parch</code></h4><p>和有无兄弟姐妹一样，同样分析可以得到</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parch_df = train_data[train_data[<span class="string">'Parch'</span>] != <span class="number">0</span>]</span><br><span class="line">no_parch_df = train_data[train_data[<span class="string">'Parch'</span>] == <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">parch_df[<span class="string">'Survived'</span>].value_counts().plot.pie(labels=[<span class="string">'No Survived'</span>, <span class="string">'Survived'</span>], autopct = <span class="string">'%1.1f%%'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'parch'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">no_parch_df[<span class="string">'Survived'</span>].value_counts().plot.pie(labels=[<span class="string">'No Survived'</span>, <span class="string">'Survived'</span>], autopct = <span class="string">'%1.1f%%'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'no_parch'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Parch_Survived.png" alt="avatar"></p>
<h4 id="亲友的人数和存活与否的关系-SibSp-amp-Parch"><a href="#亲友的人数和存活与否的关系-SibSp-amp-Parch" class="headerlink" title="亲友的人数和存活与否的关系 SibSp &amp; Parch"></a>亲友的人数和存活与否的关系 <code>SibSp &amp; Parch</code></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig,ax=plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">18</span>,<span class="number">8</span>))</span><br><span class="line">train_data[[<span class="string">'Parch'</span>,<span class="string">'Survived'</span>]].groupby([<span class="string">'Parch'</span>]).mean().plot.bar(ax=ax[<span class="number">0</span>])</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">'Parch and Survived'</span>)</span><br><span class="line">train_data[[<span class="string">'SibSp'</span>,<span class="string">'Survived'</span>]].groupby([<span class="string">'SibSp'</span>]).mean().plot.bar(ax=ax[<span class="number">1</span>])</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">'SibSp and Survived'</span>)</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">'Family_Size'</span>] = train_data[<span class="string">'Parch'</span>] + train_data[<span class="string">'SibSp'</span>] + <span class="number">1</span></span><br><span class="line">train_data[[<span class="string">'Family_Size'</span>,<span class="string">'Survived'</span>]].groupby([<span class="string">'Family_Size'</span>]).mean().plot.bar()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/SibSp_Parch_Survived.png" alt="avatar"><br><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Family_Size_Survived.png" alt="avatar"><br>从图表中可以看出，若独自一人，那么其存活率比较低；但是如果亲友太多的话，存活率也会很低。</p>
<h4 id="票价分布和存活与否的关系-Fare"><a href="#票价分布和存活与否的关系-Fare" class="headerlink" title="票价分布和存活与否的关系 Fare"></a>票价分布和存活与否的关系 <code>Fare</code></h4><p>首先绘制票价的分布情况</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">train_data[<span class="string">'Fare'</span>].hist(bins = <span class="number">70</span>)</span><br><span class="line"></span><br><span class="line">train_data.boxplot(column=<span class="string">'Fare'</span>, by=<span class="string">'Pclass'</span>, showfliers=<span class="literal">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Fare_dis1.png" alt="avatar"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[<span class="string">'Fare'</span>].describe()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Fare_describe.png" alt="avatar"></p>
<p>绘制生存与否与票价均值和方差的关系</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fare_not_survived = train_data[<span class="string">'Fare'</span>][train_data[<span class="string">'Survived'</span>] == <span class="number">0</span>]</span><br><span class="line">fare_survived = train_data[<span class="string">'Fare'</span>][train_data[<span class="string">'Survived'</span>] == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">average_fare = pd.DataFrame([fare_not_survived.mean(), fare_survived.mean()])</span><br><span class="line">std_fare = pd.DataFrame([fare_not_survived.std(), fare_survived.std()])</span><br><span class="line">average_fare.plot(yerr=std_fare, kind=<span class="string">'bar'</span>, legend=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Fare_Var.png" alt="avatar"><br>由上图标可知，票价与是否生还有一定的相关性，生还者的平均票价要大于未生还者的平均票价。</p>
<h4 id="船舱类型和存活与否的关系-Cabin"><a href="#船舱类型和存活与否的关系-Cabin" class="headerlink" title="船舱类型和存活与否的关系 Cabin"></a>船舱类型和存活与否的关系 <code>Cabin</code></h4><p>由于船舱的缺失值确实太多，有效值仅仅有204个，很难分析出不同的船舱和存活的关系，所以在做特征工程的时候，可以直接将该组特征<code>丢弃</code>。<br>简单地将数据分为是否有Cabin记录作为特征，与生存与否进行分析。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Replace missing values with "U0"</span></span><br><span class="line">train_data.loc[train_data.Cabin.isnull(), <span class="string">'Cabin'</span>] = <span class="string">'U0'</span></span><br><span class="line">train_data[<span class="string">'Has_Cabin'</span>] = train_data[<span class="string">'Cabin'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">0</span> <span class="keyword">if</span> x == <span class="string">'U0'</span> <span class="keyword">else</span> <span class="number">1</span>)</span><br><span class="line">train_data[[<span class="string">'Has_Cabin'</span>,<span class="string">'Survived'</span>]].groupby([<span class="string">'Has_Cabin'</span>]).mean().plot.bar()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Has_Cabin.png" alt="avatar"><br>对不同类型的船舱进行分析</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create feature for the alphabetical part of the cabin number</span></span><br><span class="line">train_data[<span class="string">'CabinLetter'</span>] = train_data[<span class="string">'Cabin'</span>].map(<span class="keyword">lambda</span> x: re.compile(<span class="string">"([a-zA-Z]+)"</span>).search(x).group())</span><br><span class="line"><span class="comment"># convert the distinct cabin letters with incremental integer values</span></span><br><span class="line">train_data[<span class="string">'CabinLetter'</span>] = pd.factorize(train_data[<span class="string">'CabinLetter'</span>])[<span class="number">0</span>]</span><br><span class="line">train_data[[<span class="string">'CabinLetter'</span>,<span class="string">'Survived'</span>]].groupby([<span class="string">'CabinLetter'</span>]).mean().plot.bar()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Cabin_Letter.png" alt="avatar"><br>可见，不同的船舱生存率也有不同，但是差别不大。所以在处理中，我们可以直接将特征删除。</p>
<h4 id="港口和存活与否的关系-Embarked"><a href="#港口和存活与否的关系-Embarked" class="headerlink" title="港口和存活与否的关系 Embarked"></a>港口和存活与否的关系 <code>Embarked</code></h4><p>泰坦尼克号从英国的南安普顿港出发，途径法国瑟堡和爱尔兰昆士敦，那么在昆士敦之前上船的人，有可能在瑟堡或昆士敦下船，这些人将不会遇到海难。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.countplot(<span class="string">'Embarked'</span>, hue=<span class="string">'Survived'</span>, data=train_data)</span><br><span class="line">plt.title(<span class="string">'Embarked and Survived'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Embarked.png" alt="avatar"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.factorplot(<span class="string">'Embarked'</span>, <span class="string">'Survived'</span>, data=train_data, size=<span class="number">3</span>, aspect=<span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">'Embarked and Survived rate'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Embarked2.png" alt="avatar"><br>由上可以看出，在不同的港口上船，生还率不同，C最高，Q次之，S最低。<br><strong>以上为所给出的数据特征与生还与否的分析</strong><br>据了解，泰坦尼克号上共有2224名乘客。本训练数据只给出了891名乘客的信息，如果该数据集是从总共的2224人中随机选出的，根据中心极限定理，该样本的数据也足够大，那么我们的分析结果就具有代表性；但如果不是随机选取，那么我们的分析结果就可能不太靠谱了。</p>
<h4 id="其他可能和存活与否有关系的特征"><a href="#其他可能和存活与否有关系的特征" class="headerlink" title="其他可能和存活与否有关系的特征"></a>其他可能和存活与否有关系的特征</h4><p>对于数据集中没有给出的特征信息，我们还可以联想其他可能会对模型产生影响的特征因素。如：乘客的国籍、乘客的身高、乘客的体重、乘客是否会游泳、乘客职业等等。</p>
<p>另外还有数据集中没有分析的几个特征：<code>Ticket（船票号）</code>、<code>Cabin（船舱号）</code>,这些因素的不同可能会影响乘客在船中的位置从而影响<code>逃生的顺序</code>。但是船舱号数据缺失，船票号类别大，难以分析规律，所以在后期模型融合的时候，将这些因素交由模型来决定其重要性。</p>
<h3 id="变量转换"><a href="#变量转换" class="headerlink" title="变量转换"></a>变量转换</h3><p>变量转换的目的是将数据转换为适用于模型使用的数据，不同模型接受不同类型的数据，<code>Scikit-learn</code>要求数据都是数字型<code>numeric</code>，所以我们要将一些<code>非数字型</code>的原始数据转换为<code>数字型numeric</code>。</p>
<p>所以下面对数据的转换进行介绍，以在进行特征工程的时候使用。</p>
<p>所有的数据可以分为两类：</p>
<ol>
<li>定量(Quantitative)变量可以以某种方式排序，<code>Age</code>就是一个很好的例子。</li>
<li>定性(Qualitative)变量描述了物体的某一（不能被数学表示的）方面，<code>Embarked</code>就是一个例子。</li>
</ol>
<h4 id="定性-Qualitative-转换"><a href="#定性-Qualitative-转换" class="headerlink" title="定性(Qualitative)转换"></a>定性(Qualitative)转换</h4><h5 id="Dummy-Variables"><a href="#Dummy-Variables" class="headerlink" title="Dummy Variables"></a>Dummy Variables</h5><p>就是类别变量或者二元变量，当qualitative variable是一些频繁出现的几个独立变量时，Dummy Variables比较适合使用。我们以Embarked为例，Embarked只包含三个值’S’,’C’,’Q’，我们可以使用下面的代码将其转换为dummies:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embark_dummies  = pd.get_dummies(train_data[<span class="string">'Embarked'</span>])</span><br><span class="line">train_data = train_data.join(embark_dummies)</span><br><span class="line">train_data.drop([<span class="string">'Embarked'</span>], axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embark_dummies = train_data[[<span class="string">'S'</span>, <span class="string">'C'</span>, <span class="string">'Q'</span>]]</span><br><span class="line">embark_dummies.head()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/embark_dummies.png" alt="avatar"></p>
<h5 id="Factorizing"><a href="#Factorizing" class="headerlink" title="Factorizing"></a>Factorizing</h5><p><code>dummy</code>不好处理<code>Cabin（船舱号）</code>这种<code>标称</code>属性，因为他出现的变量比较多。所以Pandas有一个方法叫做<code>factorize()</code>，它可以<strong>创建一些数字，来表示类别变量，对每一个类别映射一个ID，这种映射最后只生成一个特征</strong>，不像dummy那样生成多个特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Replace missing values with "U0"</span></span><br><span class="line">train_data[<span class="string">'Cabin'</span>][train_data.Cabin.isnull()] = <span class="string">'U0'</span></span><br><span class="line"><span class="comment"># create feature for the alphabetical part of the cabin number</span></span><br><span class="line">train_data[<span class="string">'CabinLetter'</span>] = train_data[<span class="string">'Cabin'</span>].map( <span class="keyword">lambda</span> x : re.compile(<span class="string">"([a-zA-Z]+)"</span>).search(x).group())</span><br><span class="line"><span class="comment"># convert the distinct cabin letters with incremental integer values</span></span><br><span class="line">train_data[<span class="string">'CabinLetter'</span>] = pd.factorize(train_data[<span class="string">'CabinLetter'</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">'CabinLetter'</span>].head()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/CabinLetter.png" alt="avatar"></p>
<h4 id="定量-Quantitative-转换"><a href="#定量-Quantitative-转换" class="headerlink" title="定量(Quantitative)转换"></a>定量(Quantitative)转换</h4><h5 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h5><p>Scaling可以<strong>将一个很大范围的数值映射到一个很小的范围</strong>(通常是-1 - 1，或则是0 - 1)，很多情况下我们需要将数值做Scaling使其<strong>范围大小一样</strong>，否则大范围数值特征将会由更高的权重。比如：Age的范围可能只是0-100，而income的范围可能是0-10000000，在某些对数组大小敏感的模型中会影响其结果。</p>
<p>下面对Age进行Scaling</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.size(train_data[<span class="string">'Age'</span>]) == <span class="number">891</span></span><br><span class="line"><span class="comment"># StandardScaler will subtract the mean from each value then scale to the unit variance</span></span><br><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">train_data[<span class="string">'Age_scaled'</span>] = scaler.fit_transform(train_data[<span class="string">'Age'</span>].values.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">'Age_scaled'</span>].head()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Scale_Age.png" alt="avatar"></p>
<h5 id="Binning"><a href="#Binning" class="headerlink" title="Binning"></a>Binning</h5><p>Binning通过观察“邻居”(即周围的值)<strong>将连续数据离散化</strong>。存储的值被分布到一些“桶”或“箱“”中，就像直方图的bin将数据划分成几块一样。下面的代码对Fare进行Binning。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Divide all fares into quartiles</span></span><br><span class="line">train_data[<span class="string">'Fare_bin'</span>] = pd.qcut(train_data[<span class="string">'Fare'</span>], <span class="number">5</span>)</span><br><span class="line">train_data[<span class="string">'Fare_bin'</span>].head()</span><br></pre></td></tr></table></figure>
<p><img src="Kaggle%E5%85%A5%E9%97%A8-1-Titanic/Bin_Fare.png" alt="avatar"><br>在将数据Bining化后，要么将数据factorize化，要么dummies化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># qcut() creates a new variable that identifies the quartile range, but we can't use the string</span></span><br><span class="line"><span class="comment"># so either factorize or create dummies from the result</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># factorize</span></span><br><span class="line">train_data[<span class="string">'Fare_bin_id'</span>] = pd.factorize(train_data[<span class="string">'Fare_bin'</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># dummies</span></span><br><span class="line">fare_bin_dummies_df = pd.get_dummies(train_data[<span class="string">'Fare_bin'</span>]).rename(columns=<span class="keyword">lambda</span> x: <span class="string">'Fare_'</span> + str(x))</span><br><span class="line">train_data = pd.concat([train_data, fare_bin_dummies_df], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p>在进行特征工程的时候，我们不仅需要对训练数据进行处理，还需要同时<strong>将测试数据同训练数据一起处理</strong>，使得二者具有相同的数据类型和数据分布。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df_org = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test_df_org = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line">test_df_org[<span class="string">'Survived'</span>] = <span class="number">0</span></span><br><span class="line">combined_train_test = train_df_org.append(test_df_org)</span><br><span class="line">PassengerId = test_df_org[<span class="string">'PassengerId'</span>]</span><br></pre></td></tr></table></figure>
<p>对数据进行特征工程，也就是从各项参数中提取出对输出结果有或大或小的影响的特征，将这些特征作为训练模型的依据。</p>
<p>一般来说，我们会先从含有缺失值的特征开始。</p>
<h4 id="Embarked"><a href="#Embarked" class="headerlink" title="Embarked"></a>Embarked</h4><p>因为<code>Embarked</code>项的缺失值不多，所以这里我们以<code>众数</code>来填充</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">combined_train_test[<span class="string">'Embarked'</span>].fillna(combined_train_test[<span class="string">'Embarked'</span>].mode().iloc[<span class="number">0</span>], inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>对于三种不同的港口，由上面介绍的数值转换，我们知道可以有两种特征处理方式：dummy和facrorizing。因为只有三个港口，所以我们可以直接用dummy来处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为了后面的特征分析，这里我们将 Embarked 特征进行facrorizing</span></span><br><span class="line">combined_train_test[<span class="string">'Embarked'</span>] = pd.factorize(combined_train_test[<span class="string">'Embarked'</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 pd.get_dummies 获取one-hot 编码</span></span><br><span class="line">emb_dummies_df = pd.get_dummies(combined_train_test[<span class="string">'Embarked'</span>], prefix=combined_train_test[[<span class="string">'Embarked'</span>]].columns[<span class="number">0</span>])</span><br><span class="line">combined_train_test = pd.concat([combined_train_test, emb_dummies_df], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>Scikit-learn</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow学习笔记(3)</title>
    <url>/2019/04/02/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/</url>
    <content><![CDATA[<center>使用**Neural Network反向传播算法**实现MNIST手写数字识别</center>

<h2 id="TensorFlow实现Neural-Network反向传播算法"><a href="#TensorFlow实现Neural-Network反向传播算法" class="headerlink" title="TensorFlow实现Neural Network反向传播算法"></a>TensorFlow实现Neural Network反向传播算法</h2><p>数据集前面有说，直接开始建立模型</p>
<h3 id="不含隐层的神经网络-输入层是784个神经元-输出层是10个神经元"><a href="#不含隐层的神经网络-输入层是784个神经元-输出层是10个神经元" class="headerlink" title="不含隐层的神经网络, 输入层是784个神经元, 输出层是10个神经元"></a>不含隐层的神经网络, 输入层是784个神经元, 输出层是10个神经元</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(mnist.train.images.shape, mnist.train.labels.shape)</span><br><span class="line">print(mnist.test.images.shape, mnist.test.labels.shape)</span><br><span class="line">print(mnist.validation.images.shape, mnist.validation.labels.shape)</span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">n_batch =  mnist.train.num_examples  // batch_size</span><br></pre></td></tr></table></figure>
<p>其实不含有隐藏层的就是多分类，和前面一样</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">10</span>])) </span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(x,W)+b)</span><br><span class="line">loss = tf.reduce_mean(tf.square(y-prediction))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.2</span>).minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(prediction,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br></pre></td></tr></table></figure>
<p>开始训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            batch_xs,batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">        acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images, y:mnist.test.labels&#125;) </span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Iter "</span> + str(epoch) + <span class="string">",Testing Accuracy "</span> + str(acc))</span><br></pre></td></tr></table></figure>
<p>结果如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Iter 0,Testing Accuracy 0.8321</span><br><span class="line">Iter 1,Testing Accuracy 0.8702</span><br><span class="line">Iter 2,Testing Accuracy 0.8819</span><br><span class="line">Iter 3,Testing Accuracy 0.8879</span><br><span class="line">Iter 4,Testing Accuracy 0.8941</span><br><span class="line">Iter 5,Testing Accuracy 0.8965</span><br><span class="line">Iter 6,Testing Accuracy 0.9006</span><br><span class="line">Iter 7,Testing Accuracy 0.9024</span><br><span class="line">Iter 8,Testing Accuracy 0.9043</span><br><span class="line">Iter 9,Testing Accuracy 0.905</span><br><span class="line">...</span><br><span class="line">Iter 190,Testing Accuracy 0.9288</span><br><span class="line">Iter 191,Testing Accuracy 0.9289</span><br><span class="line">Iter 192,Testing Accuracy 0.9291</span><br><span class="line">Iter 193,Testing Accuracy 0.929</span><br><span class="line">Iter 194,Testing Accuracy 0.9289</span><br><span class="line">Iter 195,Testing Accuracy 0.929</span><br><span class="line">Iter 196,Testing Accuracy 0.9287</span><br><span class="line">Iter 197,Testing Accuracy 0.9291</span><br><span class="line">Iter 198,Testing Accuracy 0.9291</span><br><span class="line">Iter 199,Testing Accuracy 0.9292</span><br></pre></td></tr></table></figure>
<p>Accuracy达到了92%以上。</p>
<h3 id="含一个隐层的神经网络"><a href="#含一个隐层的神经网络" class="headerlink" title="含一个隐层的神经网络"></a>含一个隐层的神经网络</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(mnist.train.images.shape, mnist.train.labels.shape)</span><br><span class="line">print(mnist.test.images.shape, mnist.test.labels.shape)</span><br><span class="line">print(mnist.validation.images.shape, mnist.validation.labels.shape)</span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">n_batch =  mnist.train.num_examples // batch_size</span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>下面的模型构建有所区别</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Weights_L1 = tf.Variable(tf.random_normal([<span class="number">784</span>,<span class="number">100</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">biase_L1 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">100</span>])) </span><br><span class="line">Wx_plus_b_L1 = tf.matmul(x, Weights_L1)+biase_L1 </span><br><span class="line">L1 = tf.nn.sigmoid(Wx_plus_b_L1)</span><br><span class="line"></span><br><span class="line">Weights_L2 = tf.Variable(tf.random_normal([<span class="number">100</span>,<span class="number">100</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">biase_L2 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">100</span>])) </span><br><span class="line">Wx_plus_b_L2 = tf.matmul(L1, Weights_L2)+biase_L2 </span><br><span class="line">L2 = tf.nn.sigmoid(Wx_plus_b_L2)</span><br><span class="line"></span><br><span class="line">Weights_L3 = tf.Variable(tf.random_normal([<span class="number">100</span>,<span class="number">10</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">biase_L3 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">10</span>]))</span><br><span class="line">Wx_plus_b_L3 = tf.matmul(L2,Weights_L3) + biase_L3</span><br><span class="line">prediction = tf.nn.sigmoid(Wx_plus_b_L3)</span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(y-prediction))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.2</span>).minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(prediction,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br></pre></td></tr></table></figure>
<p>值得注意的是<code>Weights_L1</code>,<code>Weights_L2</code>,<code>Weights_L3</code>,使用正态分布初始化时，<code>stddev</code>不能太大。</p>
<p>开始训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">200</span>): </span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):             </span><br><span class="line">            batch_xs,batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">        acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images, y:mnist.test.labels&#125;) </span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Iter "</span> + str(epoch) + <span class="string">",Testing Accuracy "</span> + str(acc))</span><br></pre></td></tr></table></figure>
<p>结果如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Iter 0,Testing Accuracy 0.1198</span><br><span class="line">Iter 1,Testing Accuracy 0.2264</span><br><span class="line">Iter 2,Testing Accuracy 0.2752</span><br><span class="line">Iter 3,Testing Accuracy 0.2504</span><br><span class="line">Iter 4,Testing Accuracy 0.3793</span><br><span class="line">Iter 5,Testing Accuracy 0.3175</span><br><span class="line">Iter 6,Testing Accuracy 0.3746</span><br><span class="line">Iter 7,Testing Accuracy 0.4038</span><br><span class="line">Iter 8,Testing Accuracy 0.4134</span><br><span class="line">Iter 9,Testing Accuracy 0.4354</span><br><span class="line">Iter 10,Testing Accuracy 0.4994</span><br><span class="line">Iter 11,Testing Accuracy 0.5342</span><br><span class="line">Iter 12,Testing Accuracy 0.5724</span><br><span class="line">Iter 13,Testing Accuracy 0.6077</span><br><span class="line">Iter 14,Testing Accuracy 0.6575</span><br><span class="line">Iter 15,Testing Accuracy 0.6749</span><br><span class="line">Iter 16,Testing Accuracy 0.7026</span><br><span class="line">Iter 17,Testing Accuracy 0.7267</span><br><span class="line">Iter 18,Testing Accuracy 0.7541</span><br><span class="line">Iter 19,Testing Accuracy 0.7596</span><br><span class="line">Iter 20,Testing Accuracy 0.7759</span><br><span class="line">Iter 21,Testing Accuracy 0.7895</span><br><span class="line">Iter 22,Testing Accuracy 0.8028</span><br><span class="line">...</span><br><span class="line">Iter 190,Testing Accuracy 0.94</span><br><span class="line">Iter 191,Testing Accuracy 0.9401</span><br><span class="line">Iter 192,Testing Accuracy 0.9399</span><br><span class="line">Iter 193,Testing Accuracy 0.9399</span><br><span class="line">Iter 194,Testing Accuracy 0.9404</span><br><span class="line">Iter 195,Testing Accuracy 0.9406</span><br><span class="line">Iter 196,Testing Accuracy 0.9401</span><br><span class="line">Iter 197,Testing Accuracy 0.9409</span><br><span class="line">Iter 198,Testing Accuracy 0.941</span><br><span class="line">Iter 199,Testing Accuracy 0.9408</span><br></pre></td></tr></table></figure>
<p>Accuracy达到了94%以上，比上一个模型好一些。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>在<code>TensorFlow</code>框架下有许多很方便的函数，但是对于初学者来说，在<code>MATLAB</code>中将这些算法实现一遍有利于掌握反向传播机制。</p>
<h2 id="MATLAB实现Neural-Network反向传播算法"><a href="#MATLAB实现Neural-Network反向传播算法" class="headerlink" title="MATLAB实现Neural Network反向传播算法"></a>MATLAB实现Neural Network反向传播算法</h2><center>以下内容参考Coursera上Machine Learning课后习题</center>

<h3 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h3><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">input_layer_size  = <span class="number">400</span>;  <span class="comment">% 20x20 Input Images of Digits</span></span><br><span class="line">hidden_layer_size = <span class="number">25</span>;   <span class="comment">% 25 hidden units</span></span><br><span class="line">num_labels = <span class="number">10</span>;          <span class="comment">% 10 labels, from 1 to 10  </span></span><br><span class="line">lambda = <span class="number">1</span>;</span><br></pre></td></tr></table></figure>
<p>数据是20*20的，隐层有25个神经元，10个类。<br><code>lambda</code>是正则化参数。</p>
<h3 id="Load-Training-Data"><a href="#Load-Training-Data" class="headerlink" title="Load Training Data"></a>Load Training Data</h3><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">load(<span class="string">'ex4data1.mat'</span>);</span><br><span class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<h3 id="定义CostFunction"><a href="#定义CostFunction" class="headerlink" title="定义CostFunction"></a>定义CostFunction</h3><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J grad]</span> = <span class="title">nnCostFunction</span><span class="params">(nn_params, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                   input_layer_size, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                   hidden_layer_size, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                   num_labels, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                   X, y, lambda)</span></span></span><br><span class="line"></span><br><span class="line">Theta1 = <span class="built_in">reshape</span>(nn_params(<span class="number">1</span>:hidden_layer_size * (input_layer_size + <span class="number">1</span>)), ...</span><br><span class="line">                 hidden_layer_size, (input_layer_size + <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">Theta2 = <span class="built_in">reshape</span>(nn_params((<span class="number">1</span> + (hidden_layer_size * (input_layer_size + <span class="number">1</span>))):<span class="keyword">end</span>), ...</span><br><span class="line">                 num_labels, (hidden_layer_size + <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">% Setup some useful variables</span></span><br><span class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">Theta1_grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(Theta1));</span><br><span class="line">Theta2_grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(Theta2));</span><br><span class="line"></span><br><span class="line">a1 = [<span class="built_in">ones</span>(<span class="built_in">size</span>(X,<span class="number">1</span>),<span class="number">1</span>) X];</span><br><span class="line">z2 = a1*Theta1';</span><br><span class="line">a2 = sigmoid(z2);</span><br><span class="line">a2 = [<span class="built_in">ones</span>(<span class="built_in">size</span>(a2,<span class="number">1</span>),<span class="number">1</span>) a2];</span><br><span class="line">z3 = Theta2*a2';</span><br><span class="line">a3 = sigmoid(z3);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k = <span class="number">1</span>:num_labels</span><br><span class="line">    J = J - <span class="built_in">log</span>(a3(k,:))*(y==k) - <span class="built_in">log</span>(<span class="number">1</span>-a3(k,:))*(<span class="number">1</span>-(y==k));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">SumSquare1 = sum(Theta1.^<span class="number">2</span>);</span><br><span class="line">SumSquare2 = sum(Theta2.^<span class="number">2</span>);</span><br><span class="line">J = J + (sum(SumSquare1(<span class="number">2</span>:<span class="keyword">end</span>)) + sum(SumSquare2(<span class="number">2</span>:<span class="keyword">end</span>))) * lambda / <span class="number">2</span>;</span><br><span class="line">J = J / m;</span><br><span class="line"></span><br><span class="line"><span class="comment">% backforward</span></span><br><span class="line">EYE = <span class="built_in">eye</span>(num_labels);</span><br><span class="line">y = EYE(:, y);</span><br><span class="line">delta3 = a3 - y;</span><br><span class="line">delta2 = Theta2(:,<span class="number">2</span>:<span class="keyword">end</span>)' * delta3 .* sigmoidGradient(z2)';</span><br><span class="line">Delta1 = delta2 * a1;</span><br><span class="line">Delta2 = delta3 * a2;</span><br><span class="line"></span><br><span class="line">Theta1(:,<span class="number">1</span>)=<span class="number">0</span>;</span><br><span class="line">Theta2(:,<span class="number">1</span>)=<span class="number">0</span>;</span><br><span class="line">Theta1_grad = Delta1 / m + lambda * Theta1 / m;</span><br><span class="line">Theta2_grad = Delta2 / m + lambda * Theta2 / m;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Unroll gradients</span></span><br><span class="line">grad = [Theta1_grad(:) ; Theta2_grad(:)];</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h3 id="Training-NN"><a href="#Training-NN" class="headerlink" title="Training NN"></a>Training NN</h3><h4 id="先初始化需要学习的参数"><a href="#先初始化需要学习的参数" class="headerlink" title="先初始化需要学习的参数"></a>先初始化需要学习的参数</h4><p>Initializing Neural Network Parameters</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">W</span> = <span class="title">randInitializeWeights</span><span class="params">(L_in, L_out)</span></span></span><br><span class="line">W = <span class="built_in">zeros</span>(L_out, <span class="number">1</span> + L_in);</span><br><span class="line"></span><br><span class="line">epsilon_init = <span class="number">0.12</span>;</span><br><span class="line">W = <span class="built_in">rand</span>(L_out,<span class="number">1</span> + L_in) * <span class="number">2</span> * epsilon_init - epsilon_init;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);</span><br><span class="line">initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Unroll parameters</span></span><br><span class="line">initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];</span><br></pre></td></tr></table></figure>
<p>这里需要注意初始化的目的是为了防止<code>参数同步</code>。<br>初始化完毕。</p>
<h4 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">fprintf(<span class="string">'\nTraining Neural Network... \n'</span>)</span><br><span class="line"></span><br><span class="line">options = optimset(<span class="string">'MaxIter'</span>, <span class="number">50</span>);</span><br><span class="line"></span><br><span class="line">lambda = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Create "short hand" for the cost function to be minimized</span></span><br><span class="line">costFunction = @(p) nnCostFunction(p, ...</span><br><span class="line">                                   input_layer_size, ...</span><br><span class="line">                                   hidden_layer_size, ...</span><br><span class="line">                                   num_labels, X, y, lambda);</span><br><span class="line"></span><br><span class="line">[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Obtain Theta1 and Theta2 back from nn_params</span></span><br><span class="line">Theta1 = <span class="built_in">reshape</span>(nn_params(<span class="number">1</span>:hidden_layer_size * (input_layer_size + <span class="number">1</span>)), ...</span><br><span class="line">                 hidden_layer_size, (input_layer_size + <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">Theta2 = <span class="built_in">reshape</span>(nn_params((<span class="number">1</span> + (hidden_layer_size * (input_layer_size + <span class="number">1</span>))):<span class="keyword">end</span>), ...</span><br><span class="line">                 num_labels, (hidden_layer_size + <span class="number">1</span>));</span><br></pre></td></tr></table></figure>
<h4 id="Implement-Predict"><a href="#Implement-Predict" class="headerlink" title="Implement Predict"></a>Implement Predict</h4><p>定义PredictFunction</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span> = <span class="title">predict</span><span class="params">(Theta1, Theta2, X)</span></span></span><br><span class="line"><span class="comment">% Useful values</span></span><br><span class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>);</span><br><span class="line">num_labels = <span class="built_in">size</span>(Theta2, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">p = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X, <span class="number">1</span>), <span class="number">1</span>);</span><br><span class="line">h1 = sigmoid([<span class="built_in">ones</span>(m, <span class="number">1</span>) X] * Theta1');</span><br><span class="line">h2 = sigmoid([<span class="built_in">ones</span>(m, <span class="number">1</span>) h1] * Theta2');</span><br><span class="line">[dummy, p] = <span class="built_in">max</span>(h2, [], <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>

<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">pred = predict(Theta1, Theta2, X);</span><br><span class="line"></span><br><span class="line">fprintf(<span class="string">'\nTraining Set Accuracy: %f\n'</span>, <span class="built_in">mean</span>(double(pred == y)) * <span class="number">100</span>);</span><br></pre></td></tr></table></figure>

<h2 id="全篇总结"><a href="#全篇总结" class="headerlink" title="全篇总结"></a>全篇总结</h2><ol>
<li>神经网络算法非常的灵活，激励函数多变，这里仅仅展示了一小部分。但是其中<code>隐层的个数</code>，每个隐层的<code>神经元个数</code>如何确定我仍然不解。</li>
<li>隐层的每个神经元是否有实际的数学意义？</li>
<li>所谓<code>反向传播</code>，其本质是多元微积分的<code>链式法则</code>的应用。</li>
<li>如何判定<code>CostFunction</code>一定有局部最小值？（这一部分的问题或许不在机器学习的范畴之中了吧…）</li>
<li>(疑问很多，不胜枚举)</li>
</ol>
<p>P.S.: 多推倒推倒数学公式还是很有益处的，计划下一篇写一些证明，顺便学习一下怎么在<code>Markdown</code>中插入<code>数学公式</code>!</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Neural Network</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>TensorFlow</tag>
        <tag>Gradient Descent</tag>
        <tag>MNIST</tag>
        <tag>Neural Network</tag>
        <tag>Backforward</tag>
        <tag>MATLAB</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow学习笔记(2)</title>
    <url>/2019/04/01/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/</url>
    <content><![CDATA[<center>使用**Logistic Regression**实现MNIST手写数字识别</center>

<h2 id="下载MNIST数据集"><a href="#下载MNIST数据集" class="headerlink" title="下载MNIST数据集"></a>下载MNIST数据集</h2><p>TensorFlow提供了一个<code>input_data.py</code>的文件专门用于下载mnist数据。调用方法代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(mnist.train.images.shape, mnist.train.labels.shape)</span><br><span class="line">print(mnist.test.images.shape, mnist.test.labels.shape)</span><br><span class="line">print(mnist.validation.images.shape, mnist.validation.labels.shape)</span><br></pre></td></tr></table></figure>

<h2 id="预览一下数据的格式"><a href="#预览一下数据的格式" class="headerlink" title="预览一下数据的格式"></a>预览一下数据的格式</h2><p>trainset是(55000, 784)的，对28*28的图片进行了拉直操作。<br>具体可视化方式如下</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(mnist.train.images[<span class="number">0</span>].reshape((<span class="number">28</span>,<span class="number">28</span>)))</span><br><span class="line">plt.show()</span><br><span class="line">print(mnist.train.labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<h2 id="设定参数"><a href="#设定参数" class="headerlink" title="设定参数"></a>设定参数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br></pre></td></tr></table></figure>

<h2 id="上一篇提到过的placeholder"><a href="#上一篇提到过的placeholder" class="headerlink" title="上一篇提到过的placeholder"></a>上一篇提到过的<strong>placeholder</strong></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<h2 id="需要学习的参数"><a href="#需要学习的参数" class="headerlink" title="需要学习的参数"></a>需要学习的参数</h2><p>这里把All_Theta分开只是为了好计算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Set model weights</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]), name=<span class="string">'W'</span>)</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]), name=<span class="string">'b'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="建立模型并且使用交叉熵作为CostFunction"><a href="#建立模型并且使用交叉熵作为CostFunction" class="headerlink" title="建立模型并且使用交叉熵作为CostFunction"></a>建立模型并且使用交叉熵作为CostFunction</h2><p>这里使用的是<strong>softmax</strong>函数，比<strong>sigmoid</strong>更好的用于多分类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Construct model</span></span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W)+b)</span><br><span class="line"><span class="comment"># Minimize error using cross entropy</span></span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h2 id="设置优化器"><a href="#设置优化器" class="headerlink" title="设置优化器"></a>设置优化器</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_op=tf.train.AdamOptimizer().minimize(cost)</span><br></pre></td></tr></table></figure>
<h3 id="初始化-amp-开始训练"><a href="#初始化-amp-开始训练" class="headerlink" title="初始化&amp;开始训练"></a>初始化&amp;开始训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            _,c=sess.run([train_op,cost],feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test</span></span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test accuracy'</span>,acc.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<h2 id="完整代码如下"><a href="#完整代码如下" class="headerlink" title="完整代码如下"></a>完整代码如下</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(mnist.train.images.shape, mnist.train.labels.shape)</span><br><span class="line">print(mnist.test.images.shape, mnist.test.labels.shape)</span><br><span class="line">print(mnist.validation.images.shape, mnist.validation.labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(mnist.train.images[<span class="number">0</span>].reshape((<span class="number">28</span>,<span class="number">28</span>)))</span><br><span class="line">plt.show()</span><br><span class="line">print(mnist.train.labels[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set model weights</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]), name=<span class="string">'W'</span>)</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]), name=<span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct model</span></span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W)+b)</span><br><span class="line"><span class="comment"># Minimize error using cross entropy</span></span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">train_op=tf.train.AdamOptimizer().minimize(cost)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            _,c=sess.run([train_op,cost],feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test</span></span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test accuracy'</span>,acc.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<p>运行结果:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch: 0001 cost&#x3D; 0.633361989</span><br><span class="line">Epoch: 0002 cost&#x3D; 0.352441514</span><br><span class="line">Epoch: 0003 cost&#x3D; 0.314567825</span><br><span class="line">Epoch: 0004 cost&#x3D; 0.295945743</span><br><span class="line">Epoch: 0005 cost&#x3D; 0.285345373</span><br><span class="line">Epoch: 0006 cost&#x3D; 0.277918718</span><br><span class="line">Epoch: 0007 cost&#x3D; 0.272353557</span><br><span class="line">Epoch: 0008 cost&#x3D; 0.267977367</span><br><span class="line">Epoch: 0009 cost&#x3D; 0.264319934</span><br><span class="line">Epoch: 0010 cost&#x3D; 0.261488377</span><br><span class="line">test accuracy 0.9271</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>TensorFlow</tag>
        <tag>Gradient Descent</tag>
        <tag>Logistic Regression</tag>
        <tag>MNIST</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow学习笔记</title>
    <url>/2019/03/28/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>最近在Coursera上学的 <strong>Machine Learning</strong>正好讲到<strong>Neural Network</strong></p>
<h2 id="使用TensorFlow-学习神经网络"><a href="#使用TensorFlow-学习神经网络" class="headerlink" title="使用TensorFlow 学习神经网络"></a>使用TensorFlow 学习神经网络</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>使用<strong>张量Tensor</strong>和<strong>算子Operator</strong>构建<strong>流Flow</strong><br>然后使用<strong>会话Session</strong>计算<br>意思就是说，主要分两步进行。第一步是构建Flow，第二步是执行Flow。<br>很简单，开干。</p>
<h3 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h3><h4 id="初始化权重矩阵"><a href="#初始化权重矩阵" class="headerlink" title="初始化权重矩阵"></a>初始化权重矩阵</h4><p>这里初始化了(2,3)和(3,1)的两个矩阵，其中里面的元素全部随机服从标准差为1，mean=0的正态分布。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w1 = tf.Variable(tf.random_normal((<span class="number">2</span>,<span class="number">3</span>), stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal((<span class="number">3</span>,<span class="number">1</span>), stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h4 id="构建Flow"><a href="#构建Flow" class="headerlink" title="构建Flow"></a>构建Flow</h4><p>很简单的前向传播模型，看代码就懂。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">0.7</span>, <span class="number">0.9</span>]])</span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br></pre></td></tr></table></figure>
<h4 id="创建Session"><a href="#创建Session" class="headerlink" title="创建Session"></a>创建Session</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(w1.initializer)</span><br><span class="line">sess.run(w2.initializer)</span><br><span class="line">print(sess.run(y))</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>记得关闭Session，释放资源。</p>
<h4 id="全局初始化"><a href="#全局初始化" class="headerlink" title="全局初始化"></a>全局初始化</h4><p>在Flow比较复杂的时候，神经元依赖关系复杂，使用全局初始化函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line">sess.run(init_op)</span><br></pre></td></tr></table></figure>
<p>这个函数可以自动处理变量之间依赖关系。</p>
<a id="more"></a>
<h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><p>使用TensorBoard进行数据可视化</p>
<h4 id="生成日志"><a href="#生成日志" class="headerlink" title="生成日志"></a>生成日志</h4><p>生成一个写日志的 writer，并将当前的 TensorFlow 计算图写入日志。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer = tf.summary.FileWriter(<span class="string">"log"</span>, tf.get_default_graph())</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>然后在terminal里输入</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir=~/Desktop/tf/<span class="built_in">log</span></span><br></pre></td></tr></table></figure>
<p>就可以看到可视化的Flow了</p>
<h3 id="张量Tensor"><a href="#张量Tensor" class="headerlink" title="张量Tensor"></a>张量Tensor</h3><p>在TensorFlow中，变量的声明函数<strong>tf.Variable</strong>是一个运算。<br>这个运算的输出结果是一个张量。</p>
<p>在构建机器学习模型时，我们需要区分需要被学习的<strong>参数</strong>和无法被学习的<strong>超参数</strong>。所以若声明变量时参数<strong>tainable</strong>为<strong>True</strong>，那么这个变量将会被加入到<strong>GraphKeys.TRAINABLE_VARIABLES</strong>集合中。<br>可以通过<strong>tf.trainable_variables</strong>函数得到所有需要优化的参数。</p>
<h3 id="类型Type"><a href="#类型Type" class="headerlink" title="类型Type"></a>类型Type</h3><p><strong>int</strong>, <strong>float32</strong>这种，声明时啥样就啥样。</p>
<h3 id="维度Shape"><a href="#维度Shape" class="headerlink" title="维度Shape"></a>维度Shape</h3><p>维度在运算中是可以被改变的，但是需要加入<strong>validate_shape=False</strong>参数。</p>
<h2 id="训练神经网络模型"><a href="#训练神经网络模型" class="headerlink" title="训练神经网络模型"></a>训练神经网络模型</h2><p>最常用的算法是<strong>反向传播算法backpropagation</strong><br>大体思路就是：<br>    1. 选取一部分训练数据batch<br>    2. 通过前向传播获得预测值<br>    3. 通过反向传播更新变量<br>上面这三步，就是一个迭代<strong>iteration</strong></p>
<h3 id="TensorFlow对数据的读取"><a href="#TensorFlow对数据的读取" class="headerlink" title="TensorFlow对数据的读取"></a>TensorFlow对数据的读取</h3><p>TensorFlow提供了<strong>placeholder</strong>机制用于提供输入数据，将数据通过 placeholder传入Flow即可。</p>
<h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> numpy . random <span class="keyword">import</span> RandomState</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">2</span>), name=<span class="string">'x-input'</span>)</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">1</span>), name=<span class="string">'y-input'</span>)</span><br><span class="line"></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line">y = tf.sigmoid(y)</span><br><span class="line">cross_entropy = -tf.reduce_mean(</span><br><span class="line">    y_ * tf.log(tf.clip_by_value(y, <span class="number">1e-10</span>, <span class="number">1.0</span>))</span><br><span class="line">    +(<span class="number">1</span>-y)*tf.log(tf.clip_by_value(<span class="number">1</span>-y, <span class="number">1e-10</span>, <span class="number">1.0</span>)))</span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">rdm = RandomState(<span class="number">1</span>)</span><br><span class="line">dataset_size = <span class="number">128</span></span><br><span class="line">X = rdm.rand(dataset_size, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">Y = [[int(x1+x2 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(sess.run(w1))</span><br><span class="line">    print(sess.run(w2))</span><br><span class="line">    </span><br><span class="line">    STEPS = <span class="number">5000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i * batch_size) % dataset_size</span><br><span class="line">        end = min(start+batch_size, dataset_size)</span><br><span class="line">        sess .run(train_step,feed_dict=&#123;x: X[start:end], y_: Y[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            total_cross_entropy = sess.run(cross_entropy, feed_dict=&#123;x: X, y_: Y&#125;)</span><br><span class="line">            print(<span class="string">"After %d training step (s) , cross entropy on all data is %g"</span> %(i , total_cross_entropy))</span><br><span class="line">    </span><br><span class="line">    print(sess.run(w1))</span><br><span class="line">    print(sess.run(w2))</span><br></pre></td></tr></table></figure>
<p>运行结果如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[-0.8113182   1.4845988   0.06532937]</span><br><span class="line"> [-2.4427042   0.0992484   0.5912243 ]]</span><br><span class="line">[[-0.8113182 ]</span><br><span class="line"> [ 1.4845988 ]</span><br><span class="line"> [ 0.06532937]]</span><br><span class="line">After 0 training step (s) , cross entropy on all data is 0.559635</span><br><span class="line">After 1000 training step (s) , cross entropy on all data is 0.554203</span><br><span class="line">After 2000 training step (s) , cross entropy on all data is 0.553355</span><br><span class="line">After 3000 training step (s) , cross entropy on all data is 0.553042</span><br><span class="line">After 4000 training step (s) , cross entropy on all data is 0.552882</span><br><span class="line">[[-2.5097506  3.0530994  2.779182 ]</span><br><span class="line"> [-4.0340977  1.5597332  3.26844  ]]</span><br><span class="line">[[-2.2767503]</span><br><span class="line"> [ 3.2578714]</span><br><span class="line"> [ 2.3726451]]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Neural Network</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫抓取证券时报内容</title>
    <url>/2019/03/24/%E7%88%AC%E8%99%AB%E6%8A%93%E5%8F%96%E8%AF%81%E5%88%B8%E6%97%B6%E6%8A%A5%E5%86%85%E5%AE%B9/</url>
    <content><![CDATA[<p>本次目标是抓取<a href="http://epaper.stcn.com" target="_blank" rel="noopener">证券时报电子报</a>往期内容并整理分类，为后面的数据分析做准备。</p>
<h3 id="准备工具"><a href="#准备工具" class="headerlink" title="准备工具"></a>准备工具</h3><p>Safari Develop mode, Python, Jupyter Notebook</p>
<h3 id="代码如下"><a href="#代码如下" class="headerlink" title="代码如下"></a>代码如下</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Init</span></span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">year = <span class="number">2019</span></span><br><span class="line">month = <span class="number">3</span></span><br><span class="line">day = <span class="number">23</span></span><br><span class="line"></span><br><span class="line">user_agent = <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span></span><br><span class="line">headers = &#123; <span class="string">'User-Agent'</span> : user_agent &#125;</span><br><span class="line">data = <span class="literal">None</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Generate DateString</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DateStr</span><span class="params">(year, month, day)</span>:</span></span><br><span class="line">    Date = str(year) + <span class="string">'-'</span></span><br><span class="line">    <span class="keyword">if</span> month &lt; <span class="number">10</span>:</span><br><span class="line">        Date = Date + <span class="string">'0'</span> + str(month)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Date = Date + str(month)</span><br><span class="line">    Date = Date + <span class="string">'/'</span></span><br><span class="line">    <span class="keyword">if</span> day &lt; <span class="number">10</span>:</span><br><span class="line">        Date = Date + <span class="string">'0'</span> + str(day)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Date = Date + str(day)</span><br><span class="line">    <span class="keyword">return</span> Date</span><br></pre></td></tr></table></figure>

<pre><code>2019-03/24</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Generate yyyy-mm-dd URL</span></span><br><span class="line"><span class="comment"># URL = GenURL(Date)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GenURL</span><span class="params">(Date)</span>:</span></span><br><span class="line">    url = <span class="string">'http://epaper.stcn.com/paper/zqsb/html/'</span>+ Date + <span class="string">'/node_2.htm'</span></span><br><span class="line">    <span class="keyword">return</span> url</span><br></pre></td></tr></table></figure>

<pre><code>http://epaper.stcn.com/paper/zqsb/html/2019-03/24/node_2.htm</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># content = GetContent(URL)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetContent</span><span class="params">(url)</span>:</span></span><br><span class="line">    req = urllib.request.Request(url, data, headers)</span><br><span class="line">    response = urllib.request.urlopen(req)</span><br><span class="line">    content = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br></pre></td></tr></table></figure>

<pre><code>&lt;script&gt;
window.location=&quot;/paper/zqsb/html/epaper/index/index.htm&quot;;
&lt;/script&gt;

http://epaper.stcn.com/paper/zqsb/html/2019-03/24/node_2.htm</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Analysis the paper</span></span><br><span class="line"><span class="comment"># PaperContent = GetPaper(content)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetPaper</span><span class="params">(content)</span>:</span></span><br><span class="line">    _soup = BeautifulSoup(content, <span class="string">"html.parser"</span>)</span><br><span class="line">    founder_content = _soup.find(<span class="string">"founder-content"</span>)</span><br><span class="line">    Paper = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> paper <span class="keyword">in</span> founder_content.find_all(<span class="string">"p"</span>):</span><br><span class="line">        <span class="keyword">if</span> paper.string != <span class="literal">None</span>:</span><br><span class="line">            Paper = Paper + paper.string</span><br><span class="line">    <span class="keyword">return</span> Paper</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Get Paper Link List</span></span><br><span class="line"><span class="comment"># URL = GenURL(year, month, day)</span></span><br><span class="line"><span class="comment"># content = GetContent(URL)</span></span><br><span class="line"><span class="comment"># LinkList = GetLink(URL, content)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetLink</span><span class="params">(url, content)</span>:</span></span><br><span class="line">    LinkList = []</span><br><span class="line">    soup = BeautifulSoup(content, <span class="string">"html.parser"</span>)</span><br><span class="line">    url = url[:<span class="number">50</span>]</span><br><span class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> soup.find_all(<span class="string">"a"</span>,href=re.compile(<span class="string">"^content"</span>)):</span><br><span class="line">        PaperURL = url + link.get(<span class="string">'href'</span>)</span><br><span class="line">        LinkList.append(PaperURL)</span><br><span class="line">    <span class="keyword">return</span> LinkList</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Work</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetAllArticles</span><span class="params">(year, month, day)</span>:</span></span><br><span class="line">    Date = DateStr(year, month, day)</span><br><span class="line">    URL = GenURL(Date) <span class="comment"># Generate yyyy-mm-dd Home Page</span></span><br><span class="line">    HomePageContent = GetContent(URL) <span class="comment"># Get the Content of the Home Page</span></span><br><span class="line">    LinkList = GetLink(URL, HomePageContent) <span class="comment"># Get the PaperLink from the Home Page</span></span><br><span class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> LinkList:</span><br><span class="line">        PaperContent = GetContent(link) <span class="comment"># Get the PaperContent from the link</span></span><br><span class="line">        PaperText = GetPaper(PaperContent) <span class="comment"># Convert PaperContent to PaperText</span></span><br><span class="line">        Date = Date.replace(<span class="string">'/'</span>,<span class="string">'-'</span>)</span><br><span class="line">        FileName = Date + <span class="string">'.txt'</span></span><br><span class="line">        f = open(FileName, <span class="string">'a+'</span>)</span><br><span class="line">        f.write(<span class="string">"Time : %s\n"</span> % time.ctime())</span><br><span class="line">        f.write(PaperText)</span><br><span class="line">        f.write(<span class="string">'\n'</span>)</span><br><span class="line">        f.close()</span><br><span class="line">        time.sleep(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">GetAllArticles(year, month, day)</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Web Spider</tag>
        <tag>Html</tag>
      </tags>
  </entry>
  <entry>
    <title>macOS下Spark的安装</title>
    <url>/2019/03/22/mac%E4%B8%8BSpark%E7%9A%84%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>最近参与了一个研究经济政策不确定性对股市波动影响的项目，里面对于新闻文本需要用到NLP，而且是海量文本。正好最近在投各大厂的简历，数据分析相关的岗位都要求熟悉Spark，时间有点紧，火烧眉毛了，那就来学一学。</p>
<p><strong>就不介绍啥是Spark了，直接上干货</strong></p>
<a id="more"></a>
<h2 id="Spark的安装"><a href="#Spark的安装" class="headerlink" title="Spark的安装"></a>Spark的安装</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>macOS有一点非常的好，它拥有一个无比强大的Terminal，什么事一行命令就可以搞定。</p>
<h4 id="神器Homebrew"><a href="#神器Homebrew" class="headerlink" title="神器Homebrew"></a>神器Homebrew</h4><p><a href="https://brew.sh" target="_blank" rel="noopener">Homebrew</a>是mac下的头牌神器了，要装什么一行”brew install”就可以搞定。把它当作Ubuntu下的”apt install”就好了。<br><a href="https://brew.sh" target="_blank" rel="noopener">Homebrew</a></p>
<h4 id="安装Java-JDK-1-8-并配置好环境变量"><a href="#安装Java-JDK-1-8-并配置好环境变量" class="headerlink" title="安装Java JDK 1.8, 并配置好环境变量"></a>安装Java JDK 1.8, 并配置好环境变量</h4><p>下载地址：<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">JDK8</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">~ vim .zshrc</span><br></pre></td></tr></table></figure>
<p> 在行末加上<br> <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">"/Library/Java/JavaVirtualMachines/jdk1.8.0_201.jdk/Contents/Home"</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span>"</span></span><br></pre></td></tr></table></figure><br><code>:wq</code>后<code>source ~/.zsh</code><br>在Terminal中输入<code>java -version</code>查看安装结果</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">java version <span class="string">"1.8.0_201"</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_201-b09)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)</span><br></pre></td></tr></table></figure>
<h4 id="安装Scala"><a href="#安装Scala" class="headerlink" title="安装Scala"></a>安装Scala</h4><p>使用<code>brew install scala</code>安装Scala</p>
<h3 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h3><p>使用<code>brew install apache-spark</code>安装Spark<br>配置Spark</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Spark的配置</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SPARK_PATH=<span class="string">"/usr/local/Cellar/apache-spark/2.4.0"</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$SPARK_PATH</span>/bin:<span class="variable">$PATH</span>"</span></span><br></pre></td></tr></table></figure>

<h2 id="运行Spark"><a href="#运行Spark" class="headerlink" title="运行Spark"></a>运行Spark</h2><p>在Terminal中输入<code>spark-shell</code>将得到如下信息<br><img src="mac%E4%B8%8BSpark%E7%9A%84%E5%AE%89%E8%A3%85/1.png" alt="SparkInfo"></p>
<h2 id="练练手"><a href="#练练手" class="headerlink" title="练练手"></a>练练手</h2><h3 id="使用spark-shell完成单词统计功能"><a href="#使用spark-shell完成单词统计功能" class="headerlink" title="使用spark-shell完成单词统计功能"></a>使用spark-shell完成单词统计功能</h3><p>先随便找一篇文章<br><img src="mac%E4%B8%8BSpark%E7%9A%84%E5%AE%89%E8%A3%85/2.png" alt="text"><br>保存在<code>/Users/xxx/Desktop/Chapter_1.txt</code><br>输入命令<code>spark-shell</code>进入scala编写环境<br>建立一个RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> textFile = sc.textFile(<span class="string">"file:///Users/xxx/Desktop/Chapter_1.txt"</span>)</span><br><span class="line">textFile: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = file:<span class="comment">///Users/xxx/Desktop/Chapter_1.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:24</span></span><br></pre></td></tr></table></figure>
<p>代码中通过<code>file://</code>前缀指定读取本地文件</p>
<h4 id="count-操作"><a href="#count-操作" class="headerlink" title="count()操作"></a>count()操作</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; textFile.count()     <span class="comment">// RDD 中的 item 数量，对于文本文件，就是总行数</span></span><br><span class="line">res0: <span class="type">Long</span> = <span class="number">183</span></span><br></pre></td></tr></table></figure>
<h4 id="first-操作"><a href="#first-操作" class="headerlink" title="first()操作"></a>first()操作</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; textFile.first()     <span class="comment">// RDD 中的第一个 item，对于文本文件，就是第一行内容</span></span><br><span class="line">res1: <span class="type">String</span> = <span class="type">Then</span> wear the gold hat, <span class="keyword">if</span> that will move her;</span><br></pre></td></tr></table></figure>
<h4 id="通过-filter-transformation-来返回一个新的-RDD"><a href="#通过-filter-transformation-来返回一个新的-RDD" class="headerlink" title="通过 filter transformation 来返回一个新的 RDD"></a>通过 filter transformation 来返回一个新的 RDD</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> linesWithGatsby = textFile.filter(line =&gt; line.contains(<span class="string">"Gatsby"</span>))   <span class="comment">// 筛选出包含 Gatsby 的行</span></span><br><span class="line">linesWithGatsby: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">5</span>] at filter at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; linesWithGatsby.count()     <span class="comment">// 统计行数</span></span><br><span class="line">res2: <span class="type">Long</span> = <span class="number">8</span></span><br></pre></td></tr></table></figure>
<p>寻找包含单词最多的那一行内容共有几个单词</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; textFile.map(line =&gt; line.split(<span class="string">" "</span>).size).reduce((a, b) =&gt; <span class="keyword">if</span> (a &gt; b) a <span class="keyword">else</span> b)</span><br><span class="line">res3: <span class="type">Int</span> = <span class="number">203</span></span><br></pre></td></tr></table></figure>
<p>代码首先将每一行内容 map 为一个整数，这将创建一个新的 RDD，并在这个 RDD 中执行 reduce 操作，找到最大的数。map()、reduce() 中的参数是 Scala 的函数字面量（function literals，也称为闭包 closures），并且可以使用语言特征或 Scala/Java 的库。例如，通过使用 Math.max() 函数（需要导入 Java 的 Math 库），可以使上述代码更容易理解:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> java.lang.<span class="type">Math</span> <span class="comment">//先导入Math函数</span></span><br><span class="line"><span class="keyword">import</span> java.lang.<span class="type">Math</span></span><br><span class="line"></span><br><span class="line">scala&gt; textFile.map(line =&gt; line.split(<span class="string">" "</span>).size).reduce((a, b) =&gt; <span class="type">Math</span>.max(a, b))</span><br><span class="line">res4: <span class="type">Int</span> = <span class="number">203</span></span><br></pre></td></tr></table></figure>
<p>Hadoop MapReduce 是常见的数据流模式，在 Spark 中同样可以实现（下面这个例子也就是 WordCount）:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> wordCounts = textFile.flatMap(line =&gt; line.split(<span class="string">" "</span>)).map(word =&gt; (word, <span class="number">1</span>)).reduceByKey((a, b) =&gt; a + b)   <span class="comment">// 实现单词统计</span></span><br><span class="line">wordCounts: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">10</span>] at reduceByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; wordCounts.collect()    <span class="comment">// 输出单词统计结果</span></span><br><span class="line">res5: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((paper,<span class="number">1</span>), (consoling,<span class="number">1</span>), (opening,<span class="number">1</span>), (month,,<span class="number">1</span>), (tone.,<span class="number">1</span>), (pleasant,<span class="number">2</span>), (national,<span class="number">1</span>), (wasn<span class="symbol">'t</span>,<span class="number">3</span>), (been,<span class="number">14</span>), (advice,<span class="number">1</span>), (reserve,<span class="number">1</span>), (you!<span class="string">",1), (breath,1), ("</span><span class="type">Whenever</span>,<span class="number">1</span>), (war.<span class="string">",1), (specimen,1), (knows,1), (substitute,1), (husky,1), (wind.,1), (afterward,1), (garages,,1), (instinct,1), (are,9), (revelations,1), (Western,1), (politician,,1), (shut,1), (alert,1), (porch.,2), (telephone,,1), (murmuring,1), (hear.,1), (we're,2), (away,3), (Nordics.,1), ("</span><span class="type">Don</span><span class="symbol">'t</span>,<span class="number">3</span>), (minute,<span class="number">3</span>), (high-bouncing,<span class="number">1</span>), (<span class="string">"Wake,1), (car,1), (dust,1), (going,6), (neighbor's,2), (a--of,1), (complacency,,1), (tangible,1), (Nick?,1), (them,7), (infinite,1), ("</span><span class="type">Sophisticated</span>--<span class="type">God</span>,,<span class="number">1</span>), (recently,<span class="number">1</span>), (to?<span class="string">",1), (Tom,"</span>,<span class="number">1</span>), (lived,<span class="number">1</span>), (air.,<span class="number">2</span>), (blew,<span class="number">3</span>), (again.,<span class="number">4</span>), (promises,<span class="number">1</span>), (ecstat...</span><br></pre></td></tr></table></figure>
<p>上述便是一些简单的单词统计的应用。</p>
<h4 id="退出"><a href="#退出" class="headerlink" title="退出"></a>退出</h4><p>输入<code>:quit</code>即可退出</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总体来说，mac安装Spark还是很愉快的，还有许多尚未了解的东西，一点一点的啃掉吧！</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>学习过程中参照了这位<a href="http://codingxiaxw.cn/2016/12/07/60-mac-spark/" target="_blank" rel="noopener">大佬的blog</a>安装过程有稍许不同。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>scala</tag>
        <tag>macOS</tag>
      </tags>
  </entry>
  <entry>
    <title>Arch Linux 安装笔记</title>
    <url>/2019/03/22/Arch-Linux-%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>以前需要Linux环境时，mac上能干直接mac，mac不行就Ubuntu。久闻ArchLinux大名，今天就学习一下。</p>
<h3 id="下载iso镜像"><a href="#下载iso镜像" class="headerlink" title="下载iso镜像"></a>下载iso镜像</h3><p>在宿舍是教育网，    <a href="http://mirrors.nju.edu.cn" target="_blank" rel="noopener">南大镜像站</a>、<a href="https://mirrors.tuna.tsinghua.edu.cn" target="_blank" rel="noopener">清华镜像站</a>都很快。这里我在清华镜像站下的最新版archlinux。<br>以后会专门写一篇介绍各大镜像站的使用方法。</p>
<h3 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h3><!-- ![你想输入的替代文字](Arch-Linux-安装笔记/1.png) -->
<p>不像Ubuntu，Parallels没有Arch Linux选项，这里我选择了<strong>Other Linux 3.x kernel 64-bit</strong>，默认的配置应该足够了。</p>
<a id="more"></a>
<h4 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h4><p>选择第一项<img src="Arch-Linux-%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0/1.png" alt="step1"></p>
<h5 id="开始分区"><a href="#开始分区" class="headerlink" title="开始分区"></a>开始分区</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fdisk /dev/sda</span></span><br></pre></td></tr></table></figure>
<h5 id="创建分区表"><a href="#创建分区表" class="headerlink" title="创建分区表"></a>创建分区表</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Command (m for help): n</span></span><br><span class="line"><span class="comment"># Partition type: Select (default p):</span></span><br><span class="line"><span class="comment"># Partition number (1-4, default 1):</span></span><br><span class="line"><span class="comment"># First sector (2048-209715199, default 2048):</span></span><br><span class="line"><span class="comment"># Last sector, +sectors or +size&#123;K,M,G&#125; (2048-209715199....., default 209715199): +15G</span></span><br></pre></td></tr></table></figure>

<p>建立第二个分区:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Command (m for help): n</span></span><br><span class="line"><span class="comment"># Partition type: Select (default p):</span></span><br><span class="line"><span class="comment"># Partition number (1-4, default 2):</span></span><br><span class="line"><span class="comment"># First sector (31459328-209715199, default 31459328):</span></span><br><span class="line"><span class="comment"># Last sector, +sectors or +size&#123;K,M,G&#125; (31459328-209715199....., default 209715199):</span></span><br></pre></td></tr></table></figure>
<p>现在预览下分区表:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Command (m for help): p</span></span><br></pre></td></tr></table></figure>
<p>向磁盘写入这些改动:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Command (m for help): w</span></span><br></pre></td></tr></table></figure>
<p>程序将显示如下信息:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The partition table has been altered!</span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">Syncing disks.</span><br></pre></td></tr></table></figure>
<h4 id="安装基本系统"><a href="#安装基本系统" class="headerlink" title="安装基本系统"></a>安装基本系统</h4><p>这一步需要联网</p>
<h5 id="检测网络状态"><a href="#检测网络状态" class="headerlink" title="检测网络状态"></a>检测网络状态</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ping baidu.com</span></span><br></pre></td></tr></table></figure>

<h5 id="用nano命令将清华源加在最前面"><a href="#用nano命令将清华源加在最前面" class="headerlink" title="用nano命令将清华源加在最前面"></a>用nano命令将清华源加在最前面</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nano /etc/pacman.d/mirrorlist</span></span><br></pre></td></tr></table></figure>
<p><img src="Arch-Linux-%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0/2.png" alt="mirrors"></p>
<h5 id="刷新列表-安装基本系统"><a href="#刷新列表-安装基本系统" class="headerlink" title="刷新列表, 安装基本系统"></a>刷新列表, 安装基本系统</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pacman -Syy</span></span><br><span class="line"><span class="comment"># pacstrap -i /mnt base</span></span><br><span class="line"></span><br><span class="line">Enter a selection (default-all):</span><br><span class="line">Enter a number (default=1):</span><br><span class="line">:: Proceed with installation? [Y/n] Y</span><br></pre></td></tr></table></figure>
<h5 id="生成fstab分区表"><a href="#生成fstab分区表" class="headerlink" title="生成fstab分区表"></a>生成fstab分区表</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># genfstab -U -p /mnt &gt;&gt; /mnt/etc/fstab</span></span><br></pre></td></tr></table></figure>
<h5 id="chroot-到新系统开始配置"><a href="#chroot-到新系统开始配置" class="headerlink" title="chroot 到新系统开始配置"></a>chroot 到新系统开始配置</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># arch-chroot /mnt /bin/bash</span></span><br></pre></td></tr></table></figure>

<h5 id="系统本地化，设置本地语言，地点等信息"><a href="#系统本地化，设置本地语言，地点等信息" class="headerlink" title="系统本地化，设置本地语言，地点等信息"></a>系统本地化，设置本地语言，地点等信息</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nano /etc/locale.gen</span></span><br><span class="line">en_US.UTF-8 UTF-8</span><br><span class="line">zh_CN.UTF-8 UTF-8</span><br><span class="line">zh_TW.UTF-8 UTF-8</span><br></pre></td></tr></table></figure>
<h6 id="接着执行locale-gen以生成locale讯息"><a href="#接着执行locale-gen以生成locale讯息" class="headerlink" title="接着执行locale-gen以生成locale讯息"></a>接着执行locale-gen以生成locale讯息</h6><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># locale-gen</span></span><br></pre></td></tr></table></figure>

<h6 id="创建-locale-conf-并提交本地化选项"><a href="#创建-locale-conf-并提交本地化选项" class="headerlink" title="创建 locale.conf 并提交本地化选项"></a>创建 locale.conf 并提交本地化选项</h6><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># echo LANG=en_US.UTF-8 &gt; /etc/locale.conf</span></span><br></pre></td></tr></table></figure>

<h6 id="设置时区"><a href="#设置时区" class="headerlink" title="设置时区"></a>设置时区</h6><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span></span><br></pre></td></tr></table></figure>

<h6 id="设置时间"><a href="#设置时间" class="headerlink" title="设置时间"></a>设置时间</h6><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hwclock --systohc --utc</span></span><br></pre></td></tr></table></figure>

<h6 id="设置主机名"><a href="#设置主机名" class="headerlink" title="设置主机名"></a>设置主机名</h6><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># echo 主机名 &gt; /etc/hostname</span></span><br><span class="line"><span class="comment"># nano /etc/hosts</span></span><br><span class="line"><span class="comment">#&lt;ip-address&gt; &lt;hostname.domain.org&gt; &lt;hostname&gt;</span></span><br><span class="line">127.0.0.1    localhost.localdomain  localhost 主机名  </span><br><span class="line">::1          localhost.localdomain  localhost</span><br></pre></td></tr></table></figure>

<h6 id="设置root密码"><a href="#设置root密码" class="headerlink" title="设置root密码"></a>设置root密码</h6><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># passwd</span></span><br></pre></td></tr></table></figure>

<h6 id="安装启动引导器grub"><a href="#安装启动引导器grub" class="headerlink" title="安装启动引导器grub"></a>安装启动引导器grub</h6><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pacman -S grub</span></span><br><span class="line"><span class="comment"># grub-install --target=i386-pc --recheck /dev/sda</span></span><br><span class="line"><span class="comment"># grub-mkconfig -o /boot/grub/grub.cfg</span></span><br></pre></td></tr></table></figure>

<h6 id="离开-chroot-环境"><a href="#离开-chroot-环境" class="headerlink" title="离开 chroot 环境"></a>离开 chroot 环境</h6><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># exit</span></span><br></pre></td></tr></table></figure>

<h6 id="重启计算机"><a href="#重启计算机" class="headerlink" title="重启计算机"></a>重启计算机</h6><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># reboot</span></span><br></pre></td></tr></table></figure>

<h4 id="安装图形界面"><a href="#安装图形界面" class="headerlink" title="安装图形界面"></a>安装图形界面</h4><p>使用root登入</p>
<h5 id="设置联网"><a href="#设置联网" class="headerlink" title="设置联网"></a>设置联网</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ip link</span><br></pre></td></tr></table></figure>
<p>找到网络设备, e.g. <strong>enp0s5</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ip link set enp0s3 up</span></span><br><span class="line"><span class="comment"># dhcpcd enp0s3</span></span><br><span class="line"><span class="comment"># systemctl enable dhcpcd@enp0s3.service</span></span><br></pre></td></tr></table></figure>
<p>这样使得设备自动联网</p>
<h5 id="更新软件包"><a href="#更新软件包" class="headerlink" title="更新软件包"></a>更新软件包</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pacman -Syu</span></span><br></pre></td></tr></table></figure>
<h5 id="安装xorg"><a href="#安装xorg" class="headerlink" title="安装xorg"></a>安装xorg</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pacman -S xorg</span></span><br></pre></td></tr></table></figure>
<p>一路默认</p>
<h5 id="安装驱动"><a href="#安装驱动" class="headerlink" title="安装驱动"></a>安装驱动</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pcaman -S xf86-video-vesa  虚拟机显卡驱动</span></span><br><span class="line"><span class="comment"># pacman -S alsa-utils 声卡驱动</span></span><br><span class="line"><span class="comment"># pacman -S xfce4 桌面套件</span></span><br><span class="line"><span class="comment"># pacman -S slim 登陆管理器</span></span><br><span class="line"><span class="comment"># pacman -S sudo 安装sudo</span></span><br><span class="line"><span class="comment"># pacman -S wqy-zenhei 安装中文字体</span></span><br><span class="line"><span class="comment"># useradd -m -s /bin/bash arch 添加一个普通用户"arch"</span></span><br><span class="line"><span class="comment"># passwd arch 设定密码</span></span><br><span class="line"><span class="comment"># visudo 为刚才添加的普通用户添加sudo的相关权限</span></span><br></pre></td></tr></table></figure>
<p><img src="Arch-Linux-%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0/3.png" alt="adduser1"><br><img src="Arch-Linux-%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0/4.png" alt="adduser2"><br>使用的是vi，自行查看vi的操作方法。<br>修改完:wq<br>新建.xinitrc</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#touch ~/.xinitrc</span></span><br></pre></td></tr></table></figure>
<p>内容如下<br><img src="Arch-Linux-%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0/5.png" alt="xinitrc"><br>添加执行权限<br>设置自动启动slim登陆器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sudo chmod +x ~/.xinitrc</span></span><br><span class="line"><span class="comment"># sudo systemctl enable slim.service</span></span><br></pre></td></tr></table></figure>
<p>然后reboot</p>
<h4 id="进入图形化界面"><a href="#进入图形化界面" class="headerlink" title="进入图形化界面"></a>进入图形化界面</h4><p><img src="Arch-Linux-%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0/6.png" alt="GraphicalInterface"></p>
<h3 id="基本工作完成"><a href="#基本工作完成" class="headerlink" title="基本工作完成"></a>基本工作完成</h3><h3 id="一些想法"><a href="#一些想法" class="headerlink" title="一些想法"></a>一些想法</h3><p>相较于其他自带图形化界面的发行版来说，安装是硬核了点，但是给予了用户更大的自由度。<br>适合喜欢折腾的用户。<br>最后看到图像化界面后感觉还挺爽hhh…</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ArchLinux</tag>
        <tag>Notes</tag>
      </tags>
  </entry>
</search>
